"""
1. 中心极限定理  (1)独立＋随机变量＋足够多数量 = 正态分布  (2)越接近期望，概率越大，方差越大，图越瘦高
2. 最大似然估计(mle) -- 估计概率模型方法  MLE = argmax_θ ∏(P(y_i | θ))
3. 损失函数(mse) -- 求theta的好方法,mse函数驻点就是凸函数时的最优解  MSE = (1/n) * Σ(y_i - ŷ_i)²
4. mse求导后，得到解析解公式 -- theta = (X^T * X)^(-1) * X^T * y
5. 梯度下降法 -- 求解theta的方法，梯度下降法收敛速度慢，但可以处理高维数据
6. 梯度下降法公式 -- theta = theta - α * (X^T * X * theta - X^T * y) 迭代到求出最优解
7. [正则化] -- 防止过拟合，在损失函数mse上加上正则化项，防止theta过大，系数控制在合理区间内，牺牲一部分的正确性，换取泛化能力
8. 正则化损失函数 -- (1)L2正则化损失函数 = MSE + λ * Σ(theta_i)²  (L2正则使得权重变小)  (2)L1正则化损失函数 = MSE + λ * Σ|theta_i| (L1正则使得权重变大或者为0)  (3) Elastic-Net正则化损失函数 = MSE + λ1 * Σ|theta_i| + λ2 * Σ(theta_i)² (L1和L2正则化结合)
9. 正则化损失函数求导后，得到解析解公式 -- theta = (X^T * X + λ * I)^(-1) * X^T * y
10. [归一化] -- 防止过拟合，将数据标准化，统一数量级，优化速度，公式为(x - mean) / std,减去均值出分界线，除以标准差减少部分数据对整体的影响
11. 全量梯度下降：只向最优出发，一次性处理大批量数据; 随机梯度下降：每次只处理一个样本，迭代次数多，但速度快; 小批量梯度下降：每次处理小批量数据，介于两者之间
12. [鲁棒性] -- 模型对异常值不敏感，鲁棒性好的模型，对异常值不敏感
"""

# 1. 中心极限定理
# 若随机变量独立且数量足够多，其和趋近于正态分布。
# 接近期望值的概率更大，方差越大时，分布图形越瘦高。

# 2. 最大似然估计（MLE）
# 用于估计概率模型参数的方法，通过最大化似然函数来获得参数估计。
# 形式为：MLE = argmax_θ ∏ P(y_i | θ)

# 3. 均方误差（MSE）
# 用于评估模型预测的准确性，其驻点在凸函数时为最优解。
# 定义为：MSE = (1/n) * Σ(y_i - ŷ_i)²

# 4. MSE的解析解
# 通过对MSE求导可得到参数的解析解：
# theta = (X^T * X)^(-1) * X^T * y

# 5. 梯度下降法
# 用于求解参数的方法，适合高维数据，收敛速度较慢。

# 6. 梯度下降法公式
# 参数更新公式为：
# theta = theta - α * (X^T * X * theta - X^T * y)
# 通过迭代达到最优解。

# 7. 正则化
# 用于防止过拟合，在损失函数中添加正则化项以限制参数范围，
# 从而提高模型的泛化能力。

# 8. 正则化损失函数
# (1) L2正则化
# Loss = MSE + λ * Σ(theta_i)²  # L2正则使得权重变小
# (2) L1正则化
# Loss = MSE + λ * Σ|theta_i|  # L1正则使得权重变大或者为0
# (3) Elastic-Net正则化
# Loss = MSE + λ1 * Σ|theta_i| + λ2 * Σ(theta_i)²  # L1和L2正则化结合

# 9. 正则化损失函数的解析解
# theta = (X^T * X + λ * I)^(-1) * X^T * y

# 10. 归一化
# 通过数据标准化（如均值为0，标准差为1）来防止过拟合，加快模型训练速度。
# 公式为：z = (x - mean) / std

# 11. 梯度下降类型
# - 全量梯度下降：一次性处理所有数据，速度较慢但精确。
# - 随机梯度下降：每次处理一个样本，速度快，迭代次数多。
# - 小批量梯度下降：每次处理一小部分数据，介于两者之间。

# 12. 鲁棒性
# 指模型对异常值的敏感程度，鲁棒性好的模型在遇到异常值时表现稳定。

import numpy as np

x = np.array([50, 60, 70, 80, 90, 100])
y = np.array([100, 120, 150, 200, 220, 260])

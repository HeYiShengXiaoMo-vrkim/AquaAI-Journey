## 开始学习
1. 5x3矩阵 torch.empty(5,3)
2. 随机初始化矩阵 torch.rand(5,3)
3. 全零矩阵 torch.zeros(5, 3, dtype=torch.long)
4. 直接使用数据构建 torch.tensor([5.5, 3])
5. tensor创建tensor x=x.new_ones(5,3,dtype=torch.float)          x=torch.randn_like(x, dtype=torch.float)
6. <name>.size() 获取维度信息，是一个元组
7. 加法操作 x+y        torch.add(x,y)    torch.add(x,y,out=result)     y.add_(x)
8. 改变tensor形状大小可以使用  y=x.view(16)   y=x.view(-1,8)  
9. 使用.item()获取元素值  x=tensor.randn(1) print(x.item())

## 自动微分
1. 使用requires_grad来记录他相关的变动  x=torch.ones(2,2,requires_grad=True) 
2. 使用.grad_fn操作可以打印出有requires_grad属性张量的所有操作
3. 使用.requires_grad_(True)可以改变张量的requires_grad属性
4. out.backward()  print(x.grad）打印梯度
5. 停止对跟踪历史中的自动求导with torch.no_grad():  print((x ** 2).requires_grad) 

## 神经网络
```
1. 定义一个神经网络
2. 处理输入以及调用反向传播
3. 计算损失值
4. 更新网络中的权重
前馈神经网络
import torch  基础张量库
import torch.nn as nn  神经网络模块
import torch.nn.functional as F  函数式接口(包括激活函数、池化等)

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__() 继承nn.Moudule类
        # 定义第一个卷积层，输入通道1(灰度图像)，输出通道6(6个特征图)，卷积核大小5x5
        self.conv1 = nn.Conv2d(1, 6, 5) 
        # 定义第二个卷积层，输入通道6，输出通道16，卷积核大小5x5
        self.conv2 = nn.Conv2d(6, 16, 5)
        # 第一个全连接层，输入维度16x5x5，输出维度120
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        # 后续全连接层fc2:120->84    fc3:84->10最终输出层，假设10分类
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # 2x2最大池化
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # 简写形式的2x2最大池化 
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        # -1自动计算batch_size  num_flat_features计算总特征数
        x = x.view(-1, self.num_flat_features(x))
        # 全连接层使用ReLU激活
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # 排除batch维度
        num_features = 1
        for s in size:
            num_features *= s
        return num_features  # 计算展评后的特证数 


net = Net()
print(net)
```
1. 模型可训练的参数可以通过net.parameters()返回 params=list(net.parameters())    print(len(params)
2. 将神经网络的梯度缓存器置零 net.zero_grad()  随机的梯度来进行反向传播out.backward(torch.randn(1,10))
3. 损失函数，一个损失函数需要一对输入，模型输出和目标，然后计算一个值来评估输出距离目标有多远 简单的MSE损失函数 cirterion=nn.MSELoss()  loss=cirterion(output, target)    print(loss) 
``` 计算图 
input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss
```
4. 跟踪步骤的反向传播 print(loss.grad_fn)  print(lossgrad_fn_functions[0][0])#Linear  print(loss.grad_fn.next_functions[0][0].next_functions[0][0])#Relu
5. 计算反向传播只需要使用 loss.backward()即可实现 
6. 更新神经网络参数，最简单的一种方式随机梯度下降weight = weight-learning_rate*gradient    learning_rate=0.1  for f in net.parameters():  f.data.sub_(f.grad.data*learning_rate) 
7. 也可以使用optim小包完成这些更新操作 optimizer=optim.SGD(net.parameters(), lr=0.01) 
output = net(input) 
loss = cirterion(output, input) 
loss.backward()
optimizer.step() 

# 训练图像分类器
1. 使用torchvision加载并且归一化CIFAR10的训练和测试数据集
2. 定义一个卷积神经网络
3. 定义一个损失函数
4. 在训练样本数据上训练网络
5. 在测试样本数据上测试网络
```
#1.导入库
import torch
import torchvision # torchvision加载并归一化CIFAR10数据
import torchvision.transforms as transforms

#2.导入数据和预处理
transform = transforms.Compose(  # transforms.Compose组合多个数据进行变换
    [transforms.ToTensor(),# 将pil图像或者numpy数组转换为张量，并将像素值从[0,255]转换到[0,1]
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #对每个通道进行标准化])

# 加载数据集
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
# 数据加载器 batch_size=4(每个批次包含四张图像)  shuffle=True(打乱数据顺序) num_workers使用两个子进程加速数据读取
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                         shuffle=False, num_workers=2)
# 定义了10个类别名称
classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
3.图片展示
import matplotlib.pyplot as plt
import numpy as np

# functions to show an image


def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()


# get some random training images
dataiter = iter(trainloader)
images, labels = dataiter.next()

# show images
imshow(torchvision.utils.make_grid(images))
# print labels
print(' '.join('%5s' % classes[labels[j]] for j in range(4)))

#4. 定义卷积神经网络
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
net = Net()
import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

#5. 训练网络
for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')

# 5.整个数据集的表现
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))
class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)
        c = (predicted == labels).squeeze()
        for i in range(4):
            label = labels[i]
            class_correct[label] += c[i].item()
            class_total[label] += 1


for i in range(10):
    print('Accuracy of %5s : %2d %%' % (
        classes[i], 100 * class_correct[i] / class_total[i]))

```

## 数据并行处理
```
device = torch.device("cuda:0") 
model.to(device)

mytensor = my_tensor.to(device)

model = nn.DataParallel(model)  通过使用dataparallel让模型并行运行

# 创建数据集 
class RandomDataset(Dataset): 
	def __init__(self, size, length): 
		self.len = length 
		self.data = torch.randn(length, size) 
	def __getitem__(self, index): 
		return self.data[index] 
	def __len__(self): 
		return self.len 
		rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),batch_size=batch_size, shuffle=True)

# 模型
class Model(nn.Module): 
# Our model 
def __init__(self, input_size, output_size): 
super(Model, self).__init__() 
self.fc = nn.Linear(input_size, output_size) 
def forward(self, input): 
output = self.fc(input) 
print("\tIn Model: input size", input.size(), "output size", output.size()) 
return output

# 创建模型并且数据并行处理
model = Model(input_size, output_size) 
if torch.cuda.device_count() > 1: 
	print("Let's use", torch.cuda.device_count(), "GPUs!") 
	# dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs 		                                                                                                                     									model = nn.DataParallel(model) 
	model.to(device)
	
# 运行模型
for data in rand_loader: 
	input = data.to(device) 
	output = model(input) 
	print("Outside: input size", input.size(), "output_size", output.size())
```
